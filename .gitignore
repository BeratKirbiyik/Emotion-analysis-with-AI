
import os
import numpy as np
import pandas as pd
from datasets import Dataset, Features, Value, ClassLabel
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
import evaluate
CSV_PATH = "örnek veriler.csv"  
TEXT_COL = "metin"
LABEL_COL = "etiket"
MODEL_NAME = "dbmdz/bert-base-turkish-cased"   
OUTPUT_DIR = "./sentiment-tr"                  
LABEL_NAMES = ["Olumsuz", "Nötr", "Olumlu"]

df = pd.read_csv(CSV_PATH, encoding="utf-16")

print(df.head())
features = Features({
    "text": Value("string"),
    "label": ClassLabel(names=LABEL_NAMES) 
})

dataset = Dataset.from_pandas(df, features=features)
tmp = dataset.train_test_split(test_size=0.2, stratify_by_column="label", seed=42)
splits = tmp["train"].train_test_split(test_size=0.125, stratify_by_column="label", seed=42)
dataset_dict = {
    "train": splits["train"],
    "validation": splits["test"],
    "test": tmp["test"],
}
names = dataset_dict["train"].features["label"].names
label2id = {name: i for i, name in enumerate(names)}
id2label = {i: name for i, name in enumerate(names)}
num_labels = len(names)
print("Label names:", names)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def tokenize_fn(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding=False,   # padding'i collator'a bırakıyoruz
        max_length=256,
    )

tokenized = {}
for split_name, ds in dataset_dict.items():
    tokenized[split_name] = ds.map(tokenize_fn, batched=True, remove_columns=["text"])

# =========================
# 4) Metrikler
# =========================
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels_eval = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels_eval)["accuracy"],
        "f1_macro": f1.compute(predictions=preds, references=labels_eval, average="macro")["f1"],
    }


model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id,
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=50,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,
    seed=42,
    fp16=False,  # GPU varsa True yapılabilir (AMP)
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)


train_result = trainer.train()
eval_result = trainer.evaluate()
print("Validation metrics:", eval_result)

test_result = trainer.evaluate(eval_dataset=tokenized["test"])
print("Test metrics:", test_result)

# Kaydet
os.makedirs(OUTPUT_DIR, exist_ok=True)
trainer.save_model(OUTPUT_DIR)      # model + config
tokenizer.save_pretrained(OUTPUT_DIR)

# Etiket eşleşmelerini da yazdır
with open(os.path.join(OUTPUT_DIR, "label_mapping.txt"), "w", encoding="utf-8") as f:
    for i, name in enumerate(names):
        f.write(f"{i}\t{name}\n")

print("Bitti. Model ve tokenizer kaydedildi:", OUTPUT_DIR)
